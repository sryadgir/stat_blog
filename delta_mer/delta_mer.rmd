---
title: "Learning about random effects with simulation"
author: Simon Yadgir
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    bookdown: true
    highlight: tango
    df_print: kable
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(data.table)
library(TMB)
library(ggplot2)

##sy: source some functions needed for delta model
code_root <- "~/all/repos/sideprojects/"
source(paste0(code_root, "utility/model_helper_functions.R"))
source(paste0(code_root, "delta_smooth_mer.R"))

```

# Modelling non-linear relationships

One of the assumptions of simple linear regression (or generalized-linear regression like logistic regression) is that an assumption is made about the relationship between each predictor and the outcome- that it is 'linear'. Figuring out when to allow this assumption to be broken is a much debated statistical dilemma: on one hand, if there is a true non-linear relationship, then making a linear assumption is an example of **model-mispsecification**. On the other hand, we often have noisey or sparse data, and fitting data too closely leads to **over-fitting**.

## Introducing the delta model

In this RMarkdown, I'll use **simulation** to show my solution to this problem, which I call a **delta model**. The delta model is different than most spline models because instead of smoothing over the values of the outcome as the predictor variable changes, it smoothes over the rates of change of the outcome as the predictor changes. The concept of modelling rates of change s called 'differencing' and is used in models like ARIMA. The delta model is different in that it is still modelling the value of the outcome, but it smooths the rate of change of the outcome with respect to the predictor.

This model has several nice features:

1. Nested within it are the two most extreme cases of a predictor-outcome relationship
    + The case of perfect linear relationship
    + The case of no relationship
2. It mathematically represents a common real **data-generating process**, that the slope of the predictor-outcome relationship for a given value of the predictor is related to slopes of nearby predictor values
    + Most non-linear models represent cases where the value of the outcome for a given value of the predictor is related to the value of the outcome for nearby predictor values
3. It naturally regularizes, in other words it uses the observed data to estimate how non-linear the relationship should be.

# Simulating the delta model

I'll go into some detail to show how the model works, and show simulation results to make sure I'm getting the expected result. I'll also discuss some limitations of this approach. Here's an outlines:

1. Statistical definition of the model
    + 1.a Starting with linear regression and the 'broken stick' model
2. Simulating data generated by delta smoothing
    + The unique design matrix
3. Running the model
    + 3.a Fitting the simulated data
    + 3.b The nested 'linear regression' scenario
    + 3.c The nested 'no correlation' scenario
4. Evaluating model performance
5. Comparison to other non-linear models

## 1. Statistical definition of the model

### 1.a Starting with linear regression and the 'broken stick' model
Recall that in linear regression, we estimate the rate of change of $y$ as $x_1$ changes; this value is the slope parameter $\beta_1$\*. This enforces the assumption that the relationship between $y$ and $x_1$ is linear. What if we thought that the relationship between $y$ and $x_1$ changed at some point. The point where we let the slope change is called a **knot**. We would have two slope parameters, $\beta_{1,1}$ and $\beta_{1,2}$, where the relationship of $y$ and $x_1$ is $\beta_{1,1}$ when $x_1<knot$ and is $\beta_{1,2}$ when $x_1>=knot$. This is called a "broken stick" model, and it's often referred to as the simplest spline. 


#### 1.aa Simulating broken stick data
Let's simulate data based on this model, and plot it. II'll use the steps I previously outlined to create a simulation.

First I'll make some fake observations in the `x_i` vector, and two slope parameters held in the `beta_k` vector. I'm going to say the knot is set where $x_1=1$
```{r}
n_obs <- 1000
##sy: first simulate a continuous variable x1
x_1 <- rnorm(n_obs)

##sy: create 2 beta parameters, I'm just picking two random values
beta_k <- c(0.5, 2.5)
```

##### 1.aaa Introducing an aggregator design matrix

Now I need to calculate the **linear predictor** for each row. We can't just apply the slope of each segment to values of $x_1$ that fall in that segment. I'll illustrate why. To calculate $y$ when $x_1<knot$, say $x_1=-2$:
$$
y=\beta_{1,1}*-2=0.5*-2=-1
$$
Same as with linear regression. But we wanted to calulcate $y$ when $x_1=2$, we can't just calculate 
$$
y=\beta_{1,1}*2=2.5*2=5
$$
That's because the $x_1$ relationship with $y$ hasn't been consistent.  Below I'll plot the data as if I had done the math above for every data point.

```{r}
##sy: calculate the true linear predictor using the wrong approach
bad_linpred_i <- ifelse(x_1 < 1, x_1 * beta_k[1], x_1 * beta_k[2])

p<-plot(x_1, bad_linpred_i)

```

 Notice the discontinuity. We need to account for the domain of $x_1$ where the relationship was different. I'll use an **aggregator** matrix to solve this problem. This is a matrix that sums up the effect of all segments preceeding a value of $x_1$ to arrive at the effect of $y$ given a specific value of $x_1$. Here's how we would calculate $y$ if $x_1=1$:
 
 $$
 y=\beta_{1,1}*(knot)+\beta_{1,2}*(x_1-knot)=1*1+2.5*(2-1)=1+2.5=3
 $$
We can represent this in a design matrix with two columns, $x_{1,1}$ and $x_{1,2}$, which will be multiplied by $\beta_{1,1}$ and $\beta_{1,2}$, respectively. If $x_1<knot$, then $x_{1,1}=x_1$ and $x_{1,2}=0$ (and the we get the same answer as linear regression). If $x_1>=knot$, then  $x_{1,1}=knot$ and $x_{1,2}=x_1-knot$. So we'll be adding the effect of $\beta_{1,1}$ up to the knot, and the effect of $\beta_{1,2}$ after the knot. I'll make this design matrix below, called `A_ik`, (A for aggregator, i for number of observations (rows), k for number of betas (columns)):


```{r}
A_ik = matrix(nrow = n_obs, ncol = 2)

##sy: first column is x_1 if x_1<knot, otherwise equal to knot
A_ik[, 1] = ifelse(x_1 < 1, x_1, 1)

##sy: second column is 0 if x_1<knot, otherwise equal to x_1-knot
A_ik[, 2] = ifelse(x_1 < 1, 0, x_1 - 1)

```

Let's look at some rows of `A_ik` where $x_1<knot$:

```{r}
head(A_ik[x_1 < 1, ])
```

And now for some rows where $x_1>=knot$:

```{r}
head(A_ik[x_1 >= 1, ])
```

And now let's calculate $y$ using this design matrix, and plot the result:

```{r}

linpred_i <- A_ik %*% beta_k

plot(x_1, linpred_i)

```
If this is confusing, think through some examples and do the simple algebra yourself to intuit what's going on.


As a last step, I'll add some stochastic noise to get our simulated observed data, stored in the `obs_i` vector. Then I'll plot `x_i` against `obs_i`

```{r}
##sy: add some stochastic noise
obs_i <- linpred_i + rnorm(n_obs, mean = 0, sd = 0.4)

p<-plot(x_1, obs_i)

```

#### 1.ab Run a broken stick as a linear regression

A great feature of the aggregator design matrix is that it let's us use software like ``lm()`` or ``lmer()`` to run broken stick (or other spline models), since we are just giving it some variables

For this simulation I set $\beta_{1,1}=0.5$ and $\beta_{1,2}=2.5$. You can see that the slope changes where $x_1=0$. 



#### 1.ac Fitting a broken stick model
We could run a model capturing this unique relationship in $y$ and $x_1$. But how would I create a design matrix such that I would fit two betas for a single predictor variable?  

but there are a couple issues with implementing a "broken stick" model:

1. What if we think there are more than one places where the relationship between $y$ and $x_1$ should change? Only allowing the slope to change at a single point  is a completely arbitrary decision, and those decisions typically lead to **model mis-specification**.
2. Even if we don't have other predictor variables that muddy up our data, we don't usually have a good sense of where the relationship should change. Deciding where the cut point should be is another arbitrary decision.
3. There is no relationship between $\beta_{1,1}$ and $\beta_{1,2}$. 


### 1.b Generalizing to the linear spline

Let's solve the first problem by saying we can let the slope change at more than one place. In spline-talk, each point where the slope changes is called a knot. Below I'll simulate some data with 10 true knots.
```{r}

n_obs <- 1000
n_knots <- 10
##sy: first simulate a continuous variable X_1
x_1 <- rnorm(n_obs)

##sy: create 10 beta parameters
betas <- rnorm(10)

##sy: calculate the true linear predictor using the broken stick approach
##sy: make a design matrix 
linpred_i <- ifelse(x_1 < mean(x_1), x_1 * betas[1], x_1 * betas[2])

##sy: add some stochastic noise
obs_i <- linpred_i + rnorm(n_obs, mean = 0, sd = 0.4)

p<-plot(x_1, obs_i)

```
 


As mentioned, the delta model smoothes over the rate of change of the outcome, let's call it $y$, as some continuous predictor variable, let's call it $x_1$ changes.


*For those who like calculus, $\beta_1$ can be thought of as the partial derivative of $y$ given $x_1$: $\frac{\delta y}{\delta x}$. I like thinking in terms of the basic algebra underlying calculus, so that's how I present this model.



