---
title: "Introduction to splines"
author: Simon Yadgir
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    bookdown: true
    highlight: tango
    df_print: kable
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(data.table)
library(TMB)
library(ggplot2)

##sy: source some functions needed for delta model
code_root <- "~/all/repos/sideprojects/"
source(paste0(code_root, "utility/model_helper_functions.R"))
source(paste0(code_root, "delta_smooth_mer.R"))

```

# Modelling non-linear relationships

One of the assumptions of simple linear regression (or generalized-linear regression like logistic regression) is that an assumption is made about the relationship between each predictor and the outcome- that it is 'linear'. Figuring out when to allow this assumption to be broken is a much debated statistical dilemma: on one hand, if there is a true non-linear relationship, then making a linear assumption is an example of **model-mispsecification**. On the other hand, we often have noisey or sparse data, and fitting data too closely leads to **over-fitting**.

## Introducing splines

In this RMarkdown, I'll use **simulation** to demonstrate how splines work. Splines are a broad category of modelling strategies used to capture non-linear relationships between a predictor variable and an outcome variable. There are many different spline variants, each with their own strengths and weaknesses. I'll focus on a couple basic spline variants that hopefully help build intuitive understanding for how they work.

I'll also use this RMarkdown to introduce what I call an **aggregator design matrix**, which is a tool that let's us see how splines are really just linear regression with some extra data prep. This is useful because we can then use splines in regression software that was designed to just do 'regular' linear regression.

# Simulating splines

I'll go into some detail to show how the model works, and show simulation results to make sure I'm getting the expected result. I'll also discuss some limitations of this approach. Here's an outlines:

1. Starting with linear regression and the 'broken stick' model
    + 1.a Simulating broken stick data
    + 1.b Introducing the **aggregator design matrix** (!)
    + 1.c Running a broken stick as a linear regression
2. Adding multiple knots
3. Allowing higher-order segments


## 1. Starting with linear regression and the 'broken stick' model
Recall that in linear regression, we estimate the rate of change of $y$ as $x_1$ changes; this value is the slope parameter $\beta_1$\*. This enforces the assumption that the relationship between $y$ and $x_1$ is linear. What if we thought that the relationship between $y$ and $x_1$ changed at some point. The point where we let the slope change is called a **knot**. We would have two slope parameters, $\beta_{1,1}$ and $\beta_{1,2}$, where the relationship of $y$ and $x_1$ is $\beta_{1,1}$ when $x_1<knot$ and is $\beta_{1,2}$ when $x_1>=knot$. Each piece of $x_1$ that has different slope parameters is called a **segment**; here we have two segments. This is called a "broken stick" model, and it's often referred to as the simplest spline.


### 1.a Simulating broken stick data
Let's simulate data based on this model, and plot it. II'll use the steps I previously outlined to create a simulation.

First I'll make some fake observations in the `x_i` vector, and two slope parameters held in the `beta_k` vector. I'm going to say the knot is set where $x_1=1$
```{r}
n_obs <- 1000
##sy: first simulate a continuous variable x1
x_1 <- rnorm(n_obs)

##sy: create 2 beta parameters, I'm just picking two random values
beta_k <- c(0.5, 2.5)
```

### 1.b Introducing an aggregator design matrix

Now I need to calculate the **linear predictor** for each row. We can't just apply the slope of each segment to values of $x_1$ that fall in that segment. I'll illustrate why. To calculate $y$ when $x_1<knot$, say $x_1=-2$:
$$
y=\beta_{1,1}*-2=0.5*-2=-1
$$
Same as with linear regression. But we wanted to calulcate $y$ when $x_1=2$, we can't just calculate 
$$
y=\beta_{1,1}*2=2.5*2=5
$$
That's because the $x_1$ relationship with $y$ hasn't been consistent.  Below I'll plot the data as if I had done the math above for every data point.

```{r}
##sy: calculate the true linear predictor using the wrong approach
bad_linpred_i <- ifelse(x_1 < 1, x_1 * beta_k[1], x_1 * beta_k[2])

p<-plot(x_1, bad_linpred_i)

```

 Notice the discontinuity. We need to account for the domain of $x_1$ where the relationship was different. I'll use an **aggregator** matrix to solve this problem. This is a matrix that sums up the effect of all segments preceeding a value of $x_1$ to arrive at the effect of $y$ given a specific value of $x_1$. Here's how we would calculate $y$ if $x_1=1$:
 
 $$
 y=\beta_{1,1}*(knot)+\beta_{1,2}*(x_1-knot)=1*1+2.5*(2-1)=1+2.5=3
 $$
We can represent this in a design matrix with two columns, $x_{1,1}$ and $x_{1,2}$, which will be multiplied by $\beta_{1,1}$ and $\beta_{1,2}$, respectively. If $x_1<knot$, then $x_{1,1}=x_1$ and $x_{1,2}=0$ (and the we get the same answer as linear regression). If $x_1>=knot$, then  $x_{1,1}=knot$ and $x_{1,2}=x_1-knot$. So we'll be adding the effect of $\beta_{1,1}$ up to the knot, and the effect of $\beta_{1,2}$ after the knot. I'll make this design matrix below, called `A_ik`, (A for aggregator, i for number of observations (rows), k for number of betas (columns)):


```{r}
A_ik = matrix(nrow = n_obs, ncol = 2)

##sy: first column is x_1 if x_1<knot, otherwise equal to knot
A_ik[, 1] = ifelse(x_1 < 1, x_1, 1)

##sy: second column is 0 if x_1<knot, otherwise equal to x_1-knot
A_ik[, 2] = ifelse(x_1 < 1, 0, x_1 - 1)

```

Let's look at some rows of `A_ik` where $x_1<knot$:

```{r}
head(A_ik[x_1 < 1, ])
```

And now for some rows where $x_1>=knot$:

```{r}
head(A_ik[x_1 >= 1, ])
```

And now let's calculate $y$ using this design matrix, and plot the result:

```{r}

linpred_i <- A_ik %*% beta_k

plot(x_1, linpred_i)

```
If this is confusing, think through some examples and do the simple algebra yourself to intuit what's going on.


As a last step, I'll add some stochastic noise to get our simulated observed data, stored in the `obs_i` vector. Then I'll plot `x_i` against `obs_i`

```{r}
##sy: add some stochastic noise
obs_i <- linpred_i + rnorm(n_obs, mean = 0, sd = 0.4)

p<-plot(x_1, obs_i)

```

### 1.c Running a broken stick as a linear regression

A great feature of the aggregator design matrix is that it gitlet's us use software like ``lm()`` or ``lmer()`` to run broken stick (or other spline models), since we are really just running normal linear regression on some processed variables. So let's run the model:

```{r}
##sy: first put data in a data.table
mod_data <- data.table(obs_i = c(obs_i), x_1_1 = A_ik[, 1], x_1_2 = A_ik[, 2])

##sy: run model and look at results (add "0 +" since we didn't have an intercept)
broken_stick_mod <- lm(obs_i ~ 0 + x_1_1 + x_1_2, data = mod_data)

##sy: make a table to look at estimate coefficients vs true coefs

coef_results <- data.table(par_name = c("beta_1_1", "beta_1_2"), est_par = coef(broken_stick_mod), true_par = beta_k)

print(coef_results)

```

The estimated parameters should be very close to the true values. Below we can plot the predictions, and overlay the data.

```{r}

preds <- predict(broken_stick_mod, interval = "confidence")

pred_full <- as.data.table(cbind(x_1, preds))

p <- ggplot(pred_full, aes(x = x_1, y = fit))+
  geom_point(data = mod_data, aes(y = obs_i))+
  geom_line(color = "cornflowerblue")+
  geom_ribbon(fill = "cornflowerblue", alpha = 0.6, aes(ymin = lwr, ymax = upr))+
  theme_classic()
print(p)

```
 
### 2. Adding more knots

One of the many problems with the broken stick model is that we only allow the relationship between $x_1$ and $y$ to change once.Only allowing the slope to change at a single point  is a completely arbitrary decision, and those decisions typically lead to **model mis-specification**. What if we think there are more than one places where the relationship between $y$ and $x_1$ should change? 

You may have guessed that the answer is to simply add more than 1 knot. This is an easy enough solution to think of, but slightly more difficult to implement. Mainly, we'll need to make the code that creates the aggregator matrix more flexible.

```{r}

n_obs <- 1000
n_knots <- 10
##sy: first simulate a continuous variable X_1
x_1 <- rnorm(n_obs)

##sy: create 10 beta parameters
betas <- rnorm(10)

##sy: calculate the true linear predictor using the broken stick approach
##sy: make a design matrix 
linpred_i <- ifelse(x_1 < mean(x_1), x_1 * betas[1], x_1 * betas[2])

##sy: add some stochastic noise
obs_i <- linpred_i + rnorm(n_obs, mean = 0, sd = 0.4)

p<-plot(x_1, obs_i)

```
 
### 3. Adding higher

*For those who like calculus, $\beta_1$ can be thought of as the partial derivative of $y$ given $x_1$: $\frac{\delta y}{\delta x}$. I like thinking in terms of the basic algebra underlying calculus, so that's how I present this model.



